{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "mount_file_id": "1SeXo1DiPVQxPeSSbFeNRA6lB3nTiqXGE",
   "authorship_tag": "ABX9TyNfrrypN3NOsqX9ewshztrp"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "TPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Importing libraries"
   ],
   "metadata": {
    "id": "DUCRR-u2GVUm"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install ydata_profiling"
   ],
   "metadata": {
    "id": "heYqpYAfHARN",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1696623935105,
     "user_tz": -180,
     "elapsed": 268,
     "user": {
      "displayName": "Дмитрий Камышников",
      "userId": "03871553027115189109"
     }
    }
   },
   "execution_count": 121,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Import the necessary libraries\n",
    "import pandas as pd\n",
    "from ydata_profiling import ProfileReport\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier"
   ],
   "metadata": {
    "id": "yjM8tvHGGcxg",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1696623935557,
     "user_tz": -180,
     "elapsed": 2,
     "user": {
      "displayName": "Дмитрий Камышников",
      "userId": "03871553027115189109"
     }
    }
   },
   "execution_count": 122,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data loading and preprocessing"
   ],
   "metadata": {
    "id": "b_E8ZWXXapEc"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# We train and evaluate each model.\n",
    "with open('order_cancellation_data.json', 'r') as file:\n",
    "    data = pd.read_json(file)"
   ],
   "metadata": {
    "id": "F8fXivxWawp6",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1696623939611,
     "user_tz": -180,
     "elapsed": 2294,
     "user": {
      "displayName": "Дмитрий Камышников",
      "userId": "03871553027115189109"
     }
    }
   },
   "execution_count": 135,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Profiling data\n",
    "report = ProfileReport(data)\n",
    "report.to_file('data_profile_report.html')"
   ],
   "metadata": {
    "id": "Zq4LskdTaxmM",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1696623939611,
     "user_tz": -180,
     "elapsed": 3,
     "user": {
      "displayName": "Дмитрий Камышников",
      "userId": "03871553027115189109"
     }
    }
   },
   "execution_count": 136,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "From the report it appears that the order_status column is a categorical attribute, needs to be replaced."
   ],
   "metadata": {
    "id": "islKlUjsbGQk"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Replacing the categorical feature in the 'order_status' column.\n",
    "data['order_status'] = data['order_status'].replace({'F':1, 'C':0})"
   ],
   "metadata": {
    "id": "yeJSsCPSbVJJ",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1696623939611,
     "user_tz": -180,
     "elapsed": 2,
     "user": {
      "displayName": "Дмитрий Камышников",
      "userId": "03871553027115189109"
     }
    }
   },
   "execution_count": 137,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Also, we see that the order_create_time column stores the date. It should be sorted, since we consider it to be a time sequence. And then we need to drop this column."
   ],
   "metadata": {
    "id": "bHX-L4IrbWxf"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Sorting the data by date and dropping this column.\n",
    "data.sort_values(by='order_create_time', inplace=True)\n",
    "data.drop('order_create_time', axis=1, inplace=True)"
   ],
   "metadata": {
    "id": "8Xhy0EYtbxWk",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1696623939611,
     "user_tz": -180,
     "elapsed": 2,
     "user": {
      "displayName": "Дмитрий Камышников",
      "userId": "03871553027115189109"
     }
    }
   },
   "execution_count": 138,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "total_order_items stores the number of items in the order and therefore it cannot be equal to 0. Therefore, all rows with zero values must be dropped and then get rid of the outliers, and for this purpose all rows with values greater than 95th percentile (8) must be dropped."
   ],
   "metadata": {
    "id": "MxBKOC9Fb6Ol"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Dropping rows with zero values in the \"total_order_items\" column.\n",
    "data = data[data['total_order_items'] > 0]\n",
    "# Removing outliers (dropping all values greater than the 95th percentile).\n",
    "data = data[data['total_order_items'] <= 8]"
   ],
   "metadata": {
    "id": "W2KN7TxccfrQ",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1696623940036,
     "user_tz": -180,
     "elapsed": 427,
     "user": {
      "displayName": "Дмитрий Камышников",
      "userId": "03871553027115189109"
     }
    }
   },
   "execution_count": 139,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "The \"cost(USD)\" column represents the cost of an order in USD. It has many missing values, so we need to fill them with the mean values. Additionally, there are values that are not logical - the cost of delivery is unlikely to be less than 8.\n",
    "Therefore, we can consider all orders with a cost less than $8 as outliers and drop them. Also, we need to drop all values greater than the 95th percentile."
   ],
   "metadata": {
    "id": "SRPCEtp1cso_"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# I'm filling the missing (NaN) values in the \"cost(USD)\" column with the mean values.\n",
    "data['cost(USD)'].fillna(data['cost(USD)'].mean(), inplace=True)\n",
    "# I'm dropping rows with values less than 8 in the \"cost(USD)\" column.\n",
    "data = data[data['cost(USD)'] >= 8]\n",
    "# I'm removing outliers by dropping all values that are greater than the 95th percentile in the \"cost(USD)\" column.\n",
    "data = data[data['cost(USD)'] <= 21.32]"
   ],
   "metadata": {
    "id": "q6QM91agdvUz",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1696623940036,
     "user_tz": -180,
     "elapsed": 4,
     "user": {
      "displayName": "Дмитрий Камышников",
      "userId": "03871553027115189109"
     }
    }
   },
   "execution_count": 140,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "The \"payment_type\" column represents the payment method for orders. It contains NaN values that need to be dropped. Additionally, this is a categorical feature that needs to be transformed into numerical values using one-hot encoding."
   ],
   "metadata": {
    "id": "sqZFKM9Zd1wc"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# I'm dropping rows with zero values in the \"payment_type\" column.\n",
    "data['payment_type'].dropna(inplace=True)\n",
    "\n",
    "# We will convert categorical features into numerical ones using one-hot encoding.\n",
    "data = pd.get_dummies(data, columns=['payment_type'])"
   ],
   "metadata": {
    "id": "RxaMnC3Yeflg",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1696623940036,
     "user_tz": -180,
     "elapsed": 4,
     "user": {
      "displayName": "Дмитрий Камышников",
      "userId": "03871553027115189109"
     }
    }
   },
   "execution_count": 141,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will fill in the missing values in the \"vendor_client_distance\" column with the mean values and remove outliers by dropping all rows with values greater than the 95th percentile."
   ],
   "metadata": {
    "id": "Biqb8n1Aej8R"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Filling in missing (NaN) values in the \"vendor_client_distance\" column with the mean values.\n",
    "data['vendor_client_distance'].fillna(data['vendor_client_distance'].mean(), inplace=True)\n",
    "# Removing outliers by dropping all values greater than the 95th percentile in the \"vendor_client_distance\" column.\n",
    "data = data[data['vendor_client_distance'] <= 9818]"
   ],
   "metadata": {
    "id": "cc6CvLOrfwiz",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1696623940036,
     "user_tz": -180,
     "elapsed": 3,
     "user": {
      "displayName": "Дмитрий Камышников",
      "userId": "03871553027115189109"
     }
    }
   },
   "execution_count": 142,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Filling missing (NaN) values in the \"estimated_delivery_time\" column with the mean values and removing outliers by dropping all rows with values greater than the 95th percentile."
   ],
   "metadata": {
    "id": "-vKXZYayf0Z4"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Filling missing (NaN) values in the \"estimated_delivery_time\" column with the mean values.\n",
    "data['estimated_delivery_time'].fillna(data['estimated_delivery_time'].mean(), inplace=True)\n",
    "# Removing outliers by dropping all rows with values greater than the 95th percentile in the \"estimated_delivery_time\" column.\n",
    "data = data[data['estimated_delivery_time'] <= 102]"
   ],
   "metadata": {
    "id": "EPcS2v1ogGSg",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1696623940037,
     "user_tz": -180,
     "elapsed": 4,
     "user": {
      "displayName": "Дмитрий Камышников",
      "userId": "03871553027115189109"
     }
    }
   },
   "execution_count": 143,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Filling missing (NaN) values in the \"predicted_order_preparation_time\" column with the mean values and removing outliers by dropping all rows with values greater than the 95th percentile."
   ],
   "metadata": {
    "id": "uJDWH0DxgVKs"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Filling missing (NaN) values in the \"predicted_order_preparation_time\" column with the mean values.\n",
    "data['predicted_order_preparation_time'].fillna(data['predicted_order_preparation_time'].mean(), inplace=True)\n",
    "# Removing outliers by dropping all rows with values greater than the 95th percentile in the \"predicted_order_preparation_time\" column.\n",
    "data = data[data['predicted_order_preparation_time'] <= 31]"
   ],
   "metadata": {
    "id": "8rNd_SvngrGP",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1696623940037,
     "user_tz": -180,
     "elapsed": 4,
     "user": {
      "displayName": "Дмитрий Камышников",
      "userId": "03871553027115189109"
     }
    }
   },
   "execution_count": 144,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "There is no need for the \"vendor_id\" column, so it can be dropped."
   ],
   "metadata": {
    "id": "o9EvtxdGgrnU"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "data.drop('vendor_id', axis=1, inplace=True)"
   ],
   "metadata": {
    "id": "7H10CWfKgyCk",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1696623940037,
     "user_tz": -180,
     "elapsed": 4,
     "user": {
      "displayName": "Дмитрий Камышников",
      "userId": "03871553027115189109"
     }
    }
   },
   "execution_count": 145,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "After loading and processing the data, we need to split it into features and targets."
   ],
   "metadata": {
    "id": "vF5-cmskjSoc"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# We split the data into features and the target variable.\n",
    "X = data.drop('order_status', axis=1)\n",
    "y = data[['order_status']]\n"
   ],
   "metadata": {
    "executionInfo": {
     "status": "ok",
     "timestamp": 1696623940037,
     "user_tz": -180,
     "elapsed": 4,
     "user": {
      "displayName": "Дмитрий Камышников",
      "userId": "03871553027115189109"
     }
    },
    "id": "tr6zqlWOjSoc"
   },
   "execution_count": 146,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then it is necessary to divide the data into training and test datasets in order to train and evaluate the performance of the machine learning model."
   ],
   "metadata": {
    "id": "CUO4ZCD1jSod"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Divide the data into training and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.9, shuffle=False)"
   ],
   "metadata": {
    "executionInfo": {
     "status": "ok",
     "timestamp": 1696623940037,
     "user_tz": -180,
     "elapsed": 4,
     "user": {
      "displayName": "Дмитрий Камышников",
      "userId": "03871553027115189109"
     }
    },
    "id": "twQYv1TEjSod"
   },
   "execution_count": 147,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "We decide to perform data balancing because we plan to apply logistic regression. SMOTE generates synthetic examples of minor class to balance the data and improve model performance. In this case, it is an excellent fit."
   ],
   "metadata": {
    "id": "BqDBLsiEj4vn"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Data balancing\n",
    "smote = SMOTE()\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)"
   ],
   "metadata": {
    "id": "ONWVT_Y1kRc4",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1696623940662,
     "user_tz": -180,
     "elapsed": 629,
     "user": {
      "displayName": "Дмитрий Камышников",
      "userId": "03871553027115189109"
     }
    }
   },
   "execution_count": 148,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "To ensure neutrality with respect to the weighting of different features, we apply data scaling."
   ],
   "metadata": {
    "id": "AXBWD8GFkZpE"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Scaling the data\n",
    "scaler = StandardScaler()\n",
    "X_train_resampled = scaler.fit_transform(X_train_resampled)\n",
    "X_test = scaler.transform(X_test)"
   ],
   "metadata": {
    "id": "ksx69u9wklzr",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1696623940662,
     "user_tz": -180,
     "elapsed": 3,
     "user": {
      "displayName": "Дмитрий Камышников",
      "userId": "03871553027115189109"
     }
    }
   },
   "execution_count": 149,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "We also scale on unbalanced data in order to test learning on other models that are not sensitive to balancing."
   ],
   "metadata": {
    "id": "sTRJdbaFlTo6"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Machine learning models for logistic regression"
   ],
   "metadata": {
    "id": "h6XrvqR5pnhs"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We write the first model on balanced data with L1 regularization and output the results."
   ],
   "metadata": {
    "id": "Nz3zpjuTklBN"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Create a logistic regression model with L1 regularized balanced data\n",
    "model = LogisticRegression(penalty='l1', solver='liblinear')\n",
    "model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Predicting values\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Model estimation\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print('Confusion Matrix:\\n', conf_matrix)\n",
    "print('Classification Report:\\n', class_report)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TL0RrASA2D2L",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1696623982968,
     "user_tz": -180,
     "elapsed": 42308,
     "user": {
      "displayName": "Дмитрий Камышников",
      "userId": "03871553027115189109"
     }
    },
    "outputId": "996351a1-5d15-4353-8015-d8db221af3da"
   },
   "execution_count": 150,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy: 0.60\n",
      "Confusion Matrix:\n",
      " [[1111   62]\n",
      " [3129 3750]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.26      0.95      0.41      1173\n",
      "           1       0.98      0.55      0.70      6879\n",
      "\n",
      "    accuracy                           0.60      8052\n",
      "   macro avg       0.62      0.75      0.56      8052\n",
      "weighted avg       0.88      0.60      0.66      8052\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "We write a second model on balanced data with L2 regularization."
   ],
   "metadata": {
    "id": "52W8cjGullqb"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Create a logistic regression model with regularized L2 balanced data\n",
    "model = LogisticRegression(penalty='l2', solver='liblinear')\n",
    "model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Predicting values\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Model estimation\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print('Confusion Matrix:\\n', conf_matrix)\n",
    "print('Classification Report:\\n', class_report)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VaovfDhhlpZ0",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1696623983362,
     "user_tz": -180,
     "elapsed": 398,
     "user": {
      "displayName": "Дмитрий Камышников",
      "userId": "03871553027115189109"
     }
    },
    "outputId": "ae92ae7d-469d-4e9b-e8c0-6e67065bd49e"
   },
   "execution_count": 151,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy: 0.60\n",
      "Confusion Matrix:\n",
      " [[1111   62]\n",
      " [3130 3749]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.26      0.95      0.41      1173\n",
      "           1       0.98      0.54      0.70      6879\n",
      "\n",
      "    accuracy                           0.60      8052\n",
      "   macro avg       0.62      0.75      0.56      8052\n",
      "weighted avg       0.88      0.60      0.66      8052\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "From the metrics of the results, it can be seen that there are no significant differences between the models."
   ],
   "metadata": {
    "id": "hcHFZpRMlvRW"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training other models and outputting results"
   ],
   "metadata": {
    "id": "Tg2vtEQ42GhT"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As two other models we took RandomForestClassifier and GradientBoostingClassifier. Let's train them and compare the results with each other and LogisticRegression results."
   ],
   "metadata": {
    "id": "bYpCbvdSmXfe"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Training models\n",
    "models = {\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier()\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    model.fit(X_train_resampled, y_train_resampled)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    results[model_name] = {\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Classification Report\": report,\n",
    "        \"Confusion Matrix\": conf_matrix\n",
    "    }\n",
    "\n",
    "# Вывод результатов\n",
    "for model_name, metrics in results.items():\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"Accuracy: {metrics['Accuracy']:.2f}\")\n",
    "    print(\"Classification Report:\\n\", metrics['Classification Report'])\n",
    "    print(\"Confusion Matrix:\\n\", metrics['Confusion Matrix'])\n",
    "    print(\"\\n\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6i78Z33C2Kv5",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1696624008405,
     "user_tz": -180,
     "elapsed": 25044,
     "user": {
      "displayName": "Дмитрий Камышников",
      "userId": "03871553027115189109"
     }
    },
    "outputId": "d469f0a4-7331-4d66-c7ac-9289cb147dc1"
   },
   "execution_count": 152,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-152-03b196cea20c>:10: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  model.fit(X_train_resampled, y_train_resampled)\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: Random Forest\n",
      "Accuracy: 0.99\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.98      0.97      1173\n",
      "           1       1.00      0.99      0.99      6879\n",
      "\n",
      "    accuracy                           0.99      8052\n",
      "   macro avg       0.98      0.99      0.98      8052\n",
      "weighted avg       0.99      0.99      0.99      8052\n",
      "\n",
      "Confusion Matrix:\n",
      " [[1151   22]\n",
      " [  54 6825]]\n",
      "\n",
      "\n",
      "Model: Gradient Boosting\n",
      "Accuracy: 0.99\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97      1173\n",
      "           1       1.00      0.99      1.00      6879\n",
      "\n",
      "    accuracy                           0.99      8052\n",
      "   macro avg       0.97      1.00      0.98      8052\n",
      "weighted avg       0.99      0.99      0.99      8052\n",
      "\n",
      "Confusion Matrix:\n",
      " [[1173    0]\n",
      " [  65 6814]]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "From the results of the metrics, we can see that both Random Forest and Gradient Boosting models have a very high accuracy of about 99%. They perform well on the classification task.\n",
    "\n",
    "- In both models, the high values of precision, recall and F1-score for both classes (0 and 1) indicate that the models predict both positive and negative cases well.\n",
    "\n",
    "- Confusion Matrix shows that the number of false positive and false negative predictions are minimal, which is a good indicator.\n",
    "\n",
    "Overall, both models perform well and are suitable for this classification task."
   ],
   "metadata": {
    "id": "3bbOipXsmWIz"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Comparing the results with the Logistic Regression model:\n",
    "\n",
    "- The Accuracy of Logistic Regression model is only 60%, which is significantly lower than Random Forest and Gradient Boosting models, which achieved about 99% accuracy.\n",
    "\n",
    "- The Confusion Matrix of the Logistic Regression model shows a significant number of false positive and false negative predictions, indicating the low accuracy of the model.\n",
    "\n",
    "- Precision and Recall for class 0 (negative class) of the Logistic Regression model are also low, which means that the model predicts this class poorly.\n",
    "\n",
    "- F1-score for both classes is also lower for Logistic Regression model compared to Random Forest and Gradient Boosting models.\n",
    "\n",
    "Conclusion: The Random Forest and Gradient Boosting models significantly outperform the Logistic Regression model in this classification task on all key metrics."
   ],
   "metadata": {
    "id": "EiBCIrkzpMAg"
   }
  }
 ]
}
